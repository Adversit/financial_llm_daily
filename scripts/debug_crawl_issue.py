"""
è°ƒè¯•é‡‡é›†ä»»åŠ¡ä¸­çš„ source_id ç¼ºå¤±é—®é¢˜

ç”¨äºå®šä½ crawl_tasks ä¸­ KeyError: 'source_id' çš„æ ¹æœ¬åŸå› 
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from datetime import timedelta
from typing import List, Dict, Any
from sqlalchemy.orm import Session

from src.config.settings import settings
from src.db.session import get_db
from src.models.source import Source, SourceType
from src.models.article import Article
from src.crawlers.rss_crawler import RSSCrawler
from src.crawlers.deduplicator import Deduplicator
from src.utils.time_utils import get_local_now
from src.utils.logger import logger


def test_rss_crawler_output():
    """æµ‹è¯• RSS é‡‡é›†å™¨çš„åŸå§‹è¾“å‡º"""
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 1: RSS é‡‡é›†å™¨åŸå§‹è¾“å‡º")
    logger.info("=" * 60)

    db: Session = next(get_db())

    try:
        # è·å–ç¬¬ä¸€ä¸ª RSS æº
        source = db.query(Source).filter(
            Source.type == SourceType.RSS,
            Source.enabled == True
        ).first()

        if not source:
            logger.error("æœªæ‰¾åˆ°å¯ç”¨çš„ RSS æº")
            return

        logger.info(f"æµ‹è¯•æº: {source.name} (ID: {source.id})")
        logger.info(f"URL: {source.url}")

        # åˆ›å»ºé‡‡é›†å™¨
        crawler = RSSCrawler(
            source_id=source.id,
            source_name=source.name,
            source_url=source.url,
            parser=source.parser
        )
        since = get_local_now() - timedelta(hours=24)

        # é‡‡é›†
        items = crawler.fetch(since=since)
        logger.info(f"é‡‡é›†åˆ° {len(items)} ç¯‡æ–‡ç« ")

        # æ£€æŸ¥å‰3ç¯‡æ–‡ç« çš„ç»“æ„
        for i, item in enumerate(items[:3], 1):
            logger.info(f"\n--- æ–‡ç«  {i} ---")
            logger.info(f"ç±»å‹: {type(item)}")

            if isinstance(item, dict):
                logger.info(f"å­—æ®µ: {list(item.keys())}")
                logger.info(f"source_id å­˜åœ¨: {'source_id' in item}")
                logger.info(f"source_id å€¼: {item.get('source_id', 'NOT_FOUND')}")
                logger.info(f"title: {item.get('title', 'N/A')[:50]}")
                logger.info(f"content_text é•¿åº¦: {len(item.get('content_text', '') or '')}")
            else:
                logger.info(f"å¯¹è±¡å±æ€§: {dir(item)}")
                if hasattr(item, '__dict__'):
                    logger.info(f"__dict__: {vars(item)}")

        return items

    finally:
        db.close()


def test_normalize_items(raw_items: List[Any]):
    """æµ‹è¯•è§„èŒƒåŒ–å¤„ç†"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 2: è§„èŒƒåŒ–å¤„ç†")
    logger.info("=" * 60)

    db: Session = next(get_db())

    try:
        source = db.query(Source).filter(
            Source.type == SourceType.RSS,
            Source.enabled == True
        ).first()

        # æ¨¡æ‹Ÿ _normalize_crawl_items
        from collections.abc import Mapping

        normalized: List[Dict] = []

        for idx, raw in enumerate(raw_items[:3], 1):
            logger.info(f"\n--- å¤„ç†æ–‡ç«  {idx} ---")
            item: Dict[str, Any] = None

            if isinstance(raw, Mapping):
                item = dict(raw)
                logger.info("âœ“ è¯†åˆ«ä¸º Mappingï¼Œè½¬ä¸º dict")
            elif hasattr(raw, "__dict__"):
                item = {k: v for k, v in vars(raw).items() if not k.startswith("_")}
                logger.info("âœ“ è¯†åˆ«ä¸ºå¯¹è±¡ï¼Œæå– __dict__")

            if item is None:
                logger.warning(f"âœ— æœªçŸ¥ç±»å‹: {type(raw)}")
                continue

            logger.info(f"å¤„ç†å‰å­—æ®µ: {list(item.keys())}")
            logger.info(f"å¤„ç†å‰ source_id: {item.get('source_id', 'NOT_FOUND')}")

            # è¡¥é½æ¥æºä¿¡æ¯
            item.setdefault("source_id", source.id)
            item.setdefault("source_name", source.name)

            logger.info(f"å¤„ç†åå­—æ®µ: {list(item.keys())}")
            logger.info(f"å¤„ç†å source_id: {item.get('source_id', 'NOT_FOUND')}")
            logger.info(f"source_id å€¼ç±»å‹: {type(item.get('source_id'))}")
            logger.info(f"source_id == source.id: {item.get('source_id') == source.id}")

            # canonical_url å›è½
            canonical_url = item.get("canonical_url") or item.get("url")
            if canonical_url:
                item["canonical_url"] = canonical_url

            normalized.append(item)

        logger.info(f"\nè§„èŒƒåŒ–å®Œæˆ: {len(raw_items)} -> {len(normalized)}")
        return normalized

    finally:
        db.close()


def test_deduplicator(normalized_items: List[Dict]):
    """æµ‹è¯•å»é‡å™¨"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 3: å»é‡å¤„ç†")
    logger.info("=" * 60)

    db: Session = next(get_db())

    try:
        # åŠ è½½å·²æœ‰æ•°æ®
        existing_urls = {
            url for (url,) in db.query(Article.url).filter(Article.url.isnot(None))
        }
        existing_hashes = [
            simhash for (simhash,) in db.query(Article.simhash).filter(Article.simhash.isnot(None))
        ]

        logger.info(f"å·²æœ‰ URL æ•°é‡: {len(existing_urls)}")
        logger.info(f"å·²æœ‰ SimHash æ•°é‡: {len(existing_hashes)}")

        # å»é‡å‰æ£€æŸ¥
        logger.info("\nå»é‡å‰æ£€æŸ¥:")
        for idx, item in enumerate(normalized_items[:3], 1):
            logger.info(f"æ–‡ç«  {idx}: source_id={item.get('source_id')}, keys={list(item.keys())}")

        # æ‰§è¡Œå»é‡
        deduplicator = Deduplicator()
        deduped_items = deduplicator.deduplicate(
            normalized_items[:3],  # åªå¤„ç†å‰3ç¯‡
            existing_urls=existing_urls,
            existing_hashes=existing_hashes
        )

        # å»é‡åæ£€æŸ¥
        logger.info(f"\nå»é‡åæ•°é‡: {len(normalized_items[:3])} -> {len(deduped_items)}")

        for idx, item in enumerate(deduped_items, 1):
            logger.info(f"\n--- å»é‡åæ–‡ç«  {idx} ---")
            logger.info(f"å­—æ®µ: {list(item.keys())}")
            logger.info(f"source_id å­˜åœ¨: {'source_id' in item}")
            logger.info(f"source_id å€¼: {item.get('source_id', 'NOT_FOUND')}")
            logger.info(f"source_name: {item.get('source_name', 'NOT_FOUND')}")
            logger.info(f"title: {item.get('title', 'N/A')[:50]}")
            logger.info(f"simhash: {item.get('simhash', 'NOT_FOUND')}")
            logger.info(f"dedup_key: {item.get('dedup_key', 'NOT_FOUND')[:50] if item.get('dedup_key') else 'NOT_FOUND'}")

        return deduped_items

    finally:
        db.close()


def test_prepare_for_storage(deduped_items: List[Dict]):
    """æµ‹è¯•å­˜å‚¨å‡†å¤‡"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 4: å­˜å‚¨å‡†å¤‡")
    logger.info("=" * 60)

    deduplicator = Deduplicator()

    # æ£€æŸ¥ dedup_key æ˜¯å¦å·²å­˜åœ¨
    for idx, item in enumerate(deduped_items, 1):
        logger.info(f"\n--- æ–‡ç«  {idx} ---")
        logger.info(f"å‡†å¤‡å‰ dedup_key: {item.get('dedup_key', 'NOT_FOUND')}")

        if "dedup_key" not in item:
            item["dedup_key"] = deduplicator.generate_dedup_key(item)
            logger.info(f"âœ“ ç”Ÿæˆ dedup_key: {item['dedup_key'][:50]}")
        else:
            logger.info(f"âœ“ dedup_key å·²å­˜åœ¨")

        # æœ€ç»ˆå­—æ®µæ£€æŸ¥
        logger.info(f"æœ€ç»ˆå­—æ®µ: {list(item.keys())}")
        logger.info(f"source_id: {item.get('source_id', 'NOT_FOUND')}")

    return deduped_items


def test_store_articles(final_items: List[Dict]):
    """æµ‹è¯•å­˜å‚¨é€»è¾‘ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯• 5: å­˜å‚¨é€»è¾‘ï¼ˆæ¨¡æ‹Ÿï¼Œä¸å®é™…å†™å…¥ï¼‰")
    logger.info("=" * 60)

    db: Session = next(get_db())

    try:
        source = db.query(Source).filter(
            Source.type == SourceType.RSS,
            Source.enabled == True
        ).first()

        existing_urls = {
            url for (url,) in db.query(Article.url).filter(Article.url.isnot(None))
        }

        from collections.abc import Mapping

        # æ¨¡æ‹Ÿ _store_articles çš„å‰åŠéƒ¨åˆ†
        normalized_items: List[Dict] = []

        for raw in final_items:
            if isinstance(raw, Mapping):
                normalized_items.append(dict(raw))
            else:
                logger.warning(f"æœªçŸ¥é‡‡é›†é¡¹ç±»å‹: type={type(raw)}")

        logger.info(f"è½¬æ¢ä¸º normalized_items: {len(normalized_items)} ä¸ª")

        # éå†å­˜å‚¨
        for idx, item in enumerate(normalized_items, 1):
            logger.info(f"\n--- å‡†å¤‡å­˜å‚¨æ–‡ç«  {idx} ---")

            # å…³é”®é€»è¾‘ï¼šè·å– source_id
            logger.info(f"item ç±»å‹: {type(item)}")
            logger.info(f"item å­—æ®µ: {list(item.keys())}")
            logger.info(f"'source_id' in item: {'source_id' in item}")

            # å°è¯•ä¸åŒçš„è®¿é—®æ–¹å¼
            try:
                # æ–¹å¼1: .get() å¸¦é»˜è®¤å€¼
                source_id_1 = item.get("source_id", source.id)
                logger.info(f"âœ“ .get('source_id', default): {source_id_1}")
            except Exception as e:
                logger.error(f"âœ— .get('source_id', default) å¤±è´¥: {e}")

            try:
                # æ–¹å¼2: ç›´æ¥è®¿é—®
                source_id_2 = item["source_id"]
                logger.info(f"âœ“ item['source_id']: {source_id_2}")
            except KeyError as e:
                logger.error(f"âœ— item['source_id'] KeyError: {e}")
            except Exception as e:
                logger.error(f"âœ— item['source_id'] å…¶ä»–é”™è¯¯: {e}")

            try:
                # æ–¹å¼3: æ£€æŸ¥åè®¿é—®
                if "source_id" not in item:
                    logger.warning(f"source_id ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤: {source.id}")
                    source_id_3 = source.id
                else:
                    source_id_3 = item["source_id"]
                logger.info(f"âœ“ æ£€æŸ¥åè®¿é—®: {source_id_3}")
            except Exception as e:
                logger.error(f"âœ— æ£€æŸ¥åè®¿é—®å¤±è´¥: {e}")

            # æ£€æŸ¥å…¶ä»–å¿…éœ€å­—æ®µ
            url = item.get("url")
            if not url:
                logger.warning(f"URL ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            if url in existing_urls:
                logger.info(f"URL å·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
                continue

            logger.info(f"âœ“ è¯¥æ–‡ç« å¯ä»¥å­˜å‚¨")
            logger.info(f"  - source_id: {item.get('source_id')}")
            logger.info(f"  - title: {item.get('title', '')[:50]}")
            logger.info(f"  - url: {url[:80]}")

    finally:
        db.close()


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    logger.info("ğŸ” å¼€å§‹è°ƒè¯•é‡‡é›†ä»»åŠ¡ source_id ç¼ºå¤±é—®é¢˜\n")

    # æµ‹è¯•1: é‡‡é›†å™¨åŸå§‹è¾“å‡º
    raw_items = test_rss_crawler_output()
    if not raw_items:
        logger.error("âŒ é‡‡é›†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return

    # æµ‹è¯•2: è§„èŒƒåŒ–å¤„ç†
    normalized_items = test_normalize_items(raw_items)
    if not normalized_items:
        logger.error("âŒ è§„èŒƒåŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return

    # æµ‹è¯•3: å»é‡å¤„ç†
    deduped_items = test_deduplicator(normalized_items)
    if not deduped_items:
        logger.warning("âš ï¸ å»é‡åæ— æ–‡ç« ï¼Œå¯èƒ½å…¨éƒ¨é‡å¤")
        return

    # æµ‹è¯•4: å­˜å‚¨å‡†å¤‡
    final_items = test_prepare_for_storage(deduped_items)

    # æµ‹è¯•5: å­˜å‚¨é€»è¾‘ï¼ˆæ¨¡æ‹Ÿï¼‰
    test_store_articles(final_items)

    logger.info("\n" + "=" * 60)
    logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
