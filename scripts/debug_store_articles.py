"""
ç²¾ç¡®å®šä½ _store_articles ä¸­çš„ KeyError

åœ¨ _store_articles çš„æ¯ä¸ªæ­¥éª¤éƒ½æ·»åŠ æ—¥å¿—
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from datetime import timedelta
from typing import List, Dict, Tuple
from collections.abc import Mapping
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from src.config.settings import settings
from src.db.session import get_db
from src.models.source import Source, SourceType
from src.models.article import Article, ProcessingStatus
from src.models.extraction import ExtractionQueue, QueueStatus
from src.crawlers.rss_crawler import RSSCrawler
from src.crawlers.deduplicator import Deduplicator
from src.utils.time_utils import get_local_now, to_local_naive
from src.utils.logger import logger


def debug_store_articles(
    db: Session,
    items: List[Dict],
    source: Source,
    existing_urls: set,
) -> Tuple[int, int]:
    """
    è°ƒè¯•ç‰ˆçš„ _store_articlesï¼Œæ·»åŠ è¯¦ç»†æ—¥å¿—
    """
    saved_count = 0
    queued_count = 0

    logger.info(f"=" * 80)
    logger.info(f"å¼€å§‹å­˜å‚¨ï¼Œè¾“å…¥ {len(items)} ä¸ª items")
    logger.info(f"=" * 80)

    normalized_items: List[Dict] = []

    # æ­¥éª¤1: è§„èŒƒåŒ–
    logger.info(f"\næ­¥éª¤ 1: è§„èŒƒåŒ– items")
    for idx, raw in enumerate(items, 1):
        logger.info(f"  - å¤„ç† item {idx}/{len(items)}: type={type(raw)}")

        if isinstance(raw, Mapping):
            normalized_items.append(dict(raw))
            logger.info(f"    âœ“ è½¬ä¸º dict")
        else:
            logger.warning(f"    âœ— æœªçŸ¥ç±»å‹ï¼Œè·³è¿‡: type={type(raw)}")

    logger.info(f"è§„èŒƒåŒ–å®Œæˆ: {len(items)} -> {len(normalized_items)}")

    # æ­¥éª¤2: éå†å­˜å‚¨
    logger.info(f"\næ­¥éª¤ 2: éå†å­˜å‚¨")

    for idx, item in enumerate(normalized_items, 1):
        logger.info(f"\n--- å¤„ç†æ–‡ç«  {idx}/{len(normalized_items)} ---")

        # 2.1 æ£€æŸ¥å­—å…¸ç»“æ„
        logger.info(f"å­—å…¸ç±»å‹: {type(item)}")
        logger.info(f"å­—å…¸å­—æ®µ: {list(item.keys())}")

        # 2.2 è·å– source_id
        logger.info(f"å°è¯•è·å– source_id...")
        try:
            logger.info(f"  - 'source_id' in item: {'source_id' in item}")
            logger.info(f"  - item.get('source_id'): {item.get('source_id')}")
            logger.info(f"  - source.id: {source.id}")

            source_id = item.get("source_id", source.id)
            logger.info(f"  âœ“ source_id = {source_id}")

            if "source_id" not in item:
                logger.warning(f"  âš ï¸ source_id ä¸åœ¨ item ä¸­ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        except Exception as e:
            logger.error(f"  âœ— è·å– source_id å¤±è´¥: {e}", exc_info=True)
            raise

        # 2.3 è·å– source_name
        logger.info(f"å°è¯•è·å– source_name...")
        try:
            source_name = item.get("source_name", source.name)
            logger.info(f"  âœ“ source_name = {source_name}")
        except Exception as e:
            logger.error(f"  âœ— è·å– source_name å¤±è´¥: {e}", exc_info=True)
            raise

        # 2.4 è·å– url
        logger.info(f"å°è¯•è·å– url...")
        try:
            url = item.get("url")
            logger.info(f"  âœ“ url = {url[:80] if url else None}")

            if not url:
                logger.warning(f"  âš ï¸ URL ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            if url in existing_urls:
                logger.info(f"  âš ï¸ URL å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
        except Exception as e:
            logger.error(f"  âœ— è·å– url å¤±è´¥: {e}", exc_info=True)
            raise

        # 2.5 è·å– published_at
        logger.info(f"å°è¯•è·å– published_at...")
        try:
            published_at = item.get("published_at")
            if published_at:
                published_at = to_local_naive(published_at)
            logger.info(f"  âœ“ published_at = {published_at}")
        except Exception as e:
            logger.error(f"  âœ— è·å– published_at å¤±è´¥: {e}", exc_info=True)
            raise

        # 2.6 åˆ›å»º Article å¯¹è±¡
        logger.info(f"å°è¯•åˆ›å»º Article å¯¹è±¡...")
        try:
            article = Article(
                source_id=source_id,
                title=item.get("title", "").strip()[:500],
                url=url,
                published_at=published_at,
                content_text=item.get("content_text", ""),
                content_len=len(item.get("content_text", "") or ""),
                canonical_url=item.get("canonical_url") or url,
                dedup_key=item.get("dedup_key"),
                simhash=item.get("simhash"),
                processing_status=ProcessingStatus.RAW,
            )
            logger.info(f"  âœ“ Article å¯¹è±¡åˆ›å»ºæˆåŠŸ")
            logger.info(f"    - article.source_id = {article.source_id}")
            logger.info(f"    - article.title = {article.title[:50]}")
        except Exception as e:
            logger.error(f"  âœ— åˆ›å»º Article å¤±è´¥: {e}", exc_info=True)
            logger.error(f"  å‚æ•°:")
            logger.error(f"    - source_id: {source_id}")
            logger.error(f"    - title: {item.get('title', '')[:50]}")
            logger.error(f"    - url: {url[:80] if url else None}")
            raise

        # 2.7 æ•°æ®åº“æ“ä½œ
        logger.info(f"å°è¯•å†™å…¥æ•°æ®åº“...")
        try:
            logger.info(f"  - æ‰§è¡Œ db.add(article)...")
            db.add(article)
            logger.info(f"  - æ‰§è¡Œ db.flush()...")
            db.flush()  # è·å– article.id
            logger.info(f"  âœ“ Article å†™å…¥æˆåŠŸ, ID={article.id}")

            queue_item = ExtractionQueue(
                article_id=article.id,
                status=QueueStatus.QUEUED,
                priority=0,
                attempts=0,
            )
            db.add(queue_item)
            logger.info(f"  âœ“ ExtractionQueue åˆ›å»ºæˆåŠŸ")

            db.commit()
            logger.info(f"  âœ“ æäº¤æˆåŠŸ")

            saved_count += 1
            queued_count += 1
            existing_urls.add(url)

        except IntegrityError as exc:
            logger.warning(f"  âš ï¸ å†™å…¥å¤±è´¥ï¼ˆå¯èƒ½é‡å¤ï¼‰: {exc}")
            db.rollback()
        except Exception as exc:
            logger.error(f"  âœ— å†™å…¥å¼‚å¸¸: {exc}", exc_info=True)
            db.rollback()

    logger.info(f"\n" + "=" * 80)
    logger.info(f"å­˜å‚¨å®Œæˆ: saved={saved_count}, queued={queued_count}")
    logger.info(f"=" * 80)

    return saved_count, queued_count


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    logger.info("ğŸ” è°ƒè¯• _store_articles å‡½æ•°\n")

    db: Session = next(get_db())

    try:
        # è·å–ç¬¬ä¸€ä¸ª RSS æº
        source = db.query(Source).filter(
            Source.type == SourceType.RSS,
            Source.enabled == True
        ).first()

        if not source:
            logger.error("æœªæ‰¾åˆ°å¯ç”¨çš„ RSS æº")
            return

        logger.info(f"æµ‹è¯•æº: {source.name} (ID: {source.id})")

        # é‡‡é›†
        crawler = RSSCrawler(
            source_id=source.id,
            source_name=source.name,
            source_url=source.url,
            parser=source.parser
        )
        since = get_local_now() - timedelta(hours=24)
        items = crawler.fetch(since=since)

        logger.info(f"é‡‡é›†åˆ° {len(items)} ç¯‡æ–‡ç« ")

        if not items:
            logger.warning("æ— æ–‡ç« ï¼Œé€€å‡º")
            return

        # è§„èŒƒåŒ–
        from collections.abc import Mapping
        normalized: List[Dict] = []
        for raw in items:
            if isinstance(raw, Mapping):
                item = dict(raw)
                item.setdefault("source_id", source.id)
                item.setdefault("source_name", source.name)
                canonical_url = item.get("canonical_url") or item.get("url")
                if canonical_url:
                    item["canonical_url"] = canonical_url
                normalized.append(item)

        logger.info(f"è§„èŒƒåŒ–å: {len(normalized)} ç¯‡")

        # å»é‡
        existing_urls = {
            url for (url,) in db.query(Article.url).filter(Article.url.isnot(None))
        }
        existing_hashes = [
            simhash for (simhash,) in db.query(Article.simhash).filter(Article.simhash.isnot(None))
        ]

        deduplicator = Deduplicator()
        items = deduplicator.deduplicate(normalized, existing_urls=existing_urls, existing_hashes=existing_hashes)

        logger.info(f"å»é‡å: {len(items)} ç¯‡")

        if not items:
            logger.warning("å»é‡åæ— æ–‡ç« ï¼Œé€€å‡º")
            return

        # å‡†å¤‡å­˜å‚¨
        for item in items:
            if "dedup_key" not in item:
                item["dedup_key"] = deduplicator.generate_dedup_key(item)

        # è°ƒè¯•å­˜å‚¨
        saved, queued = debug_store_articles(db, items, source, existing_urls)

        logger.info(f"\nâœ… æµ‹è¯•å®Œæˆ: saved={saved}, queued={queued}")

    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e)}")
        logger.error(f"é”™è¯¯å‚æ•°: {e.args}")

        # æ‰“å°å®Œæ•´å †æ ˆ
        import traceback
        logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")

    finally:
        db.close()


if __name__ == "__main__":
    main()
